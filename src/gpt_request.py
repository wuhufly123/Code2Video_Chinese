import openai
import time
import random
import os
import base64
from openai import OpenAI
import time
import json
import pathlib


# Read and cache once
_CFG_PATH = pathlib.Path(__file__).with_name("api_config.json")
with _CFG_PATH.open("r", encoding="utf-8") as _f:
    _CFG = json.load(_f)


def cfg(svc: str, key: str, default=None):
    return os.getenv(f"{svc}_{key}".upper(), _CFG.get(svc, {}).get(key, default))


def generate_log_id():
    """Generate a log ID with 'tkb' prefix and current timestamp."""
    return f"tkb{int(time.time() * 1000)}"


def request_claude(prompt, log_id=None, max_tokens=16384, max_retries=3):
    base_url = cfg("claude", "base_url")
    api_key = cfg("claude", "api_key")
    model_name = cfg("claude", "model")
    client = OpenAI(base_url=base_url, api_key=api_key)

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}

    retry_count = 0
    while retry_count < max_retries:
        try:
            response = client.chat.completions.create(
                model = model_name,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt,
                            },
                        ],
                    }
                ],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")

            # Exponential backoff with jitter
            delay = (2**retry_count) * 0.1 + (random.random() * 0.1)
            print(
                f"Request failed with error: {str(e)}. Retrying in {delay:.2f} seconds... (Attempt {retry_count}/{max_retries})"
            )
            time.sleep(delay)


def request_claude_token(prompt, log_id=None, max_tokens=10000, max_retries=3):
    base_url = cfg("claude", "base_url")
    api_key = cfg("claude", "api_key")
    client = OpenAI(base_url=base_url, api_key=api_key)
    model_name = cfg("claude", "model")
    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}
    usage_info = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt,
                            },
                        ],
                    }
                ],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
            )
            # --- MODIFIED: token usage ---
            if completion.usage:
                usage_info["prompt_tokens"] = completion.usage.prompt_tokens
                usage_info["completion_tokens"] = completion.usage.completion_tokens
                usage_info["total_tokens"] = completion.usage.total_tokens
            return completion, usage_info

        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")

            # Exponential backoff with jitter
            delay = (2**retry_count) * 0.1 + (random.random() * 0.1)
            print(
                f"Request failed with error: {str(e)}. Retrying in {delay:.2f} seconds... (Attempt {retry_count}/{max_retries})"
            )
            time.sleep(delay)

    return None, usage_info

def request_gemini_with_video(prompt: str, video_path: str, log_id=None, max_tokens: int = 10000, max_retries: int = 10):
    """
    Makes a multimodal request to the Gemini model using video + text via OpenAI-compatible proxy.
    """
    base_url = cfg("gemini", "base_url")
    # api_version = cfg("gemini", "api_version") # Standard OpenAI proxy usually doesn't need api_version in init
    api_key = cfg("gemini", "api_key")
    model_name = cfg("gemini", "model")

    # ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ base_url åˆå§‹åŒ–æ ‡å‡† OpenAI å®¢æˆ·ç«¯
    client = OpenAI(
        base_url=base_url,
        api_key=api_key,
        timeout=300.0,
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}

    # Load and base64-encode video
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"Video not found: {video_path}")

    with open(video_path, "rb") as f:
        video_bytes = f.read()

    video_base64 = base64.b64encode(video_bytes).decode("utf-8")
    data_url = f"data:video/mp4;base64,{video_base64}"

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image_url", "image_url": {"url": data_url, "detail": "high"}, "media_type": "video/mp4"},
                        ],
                    }
                ],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
            )
            return completion

        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")
            delay = (2**retry_count) * 0.2 + random.random() * 0.2
            print(f"Retry {retry_count}/{max_retries} after error: {e}, waiting {delay:.2f}s...")
            time.sleep(delay)


def request_gemini_video_img(
    prompt: str, video_path: str, image_path: str, log_id=None, max_tokens: int = 10000, max_retries: int = 10
):
    """
    Makes a multimodal request to the Gemini model using video & ref img + text via OpenAI-compatible proxy.
    """
    base_url = cfg("gemini", "base_url")
    # api_version = cfg("gemini", "api_version")
    api_key = cfg("gemini", "api_key")
    model_name = cfg("gemini", "model")

    # ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ base_url åˆå§‹åŒ–æ ‡å‡† OpenAI å®¢æˆ·ç«¯
    client = OpenAI(
        base_url=base_url,
        api_key=api_key,
        timeout=300.0,
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}

    # Load and base64-encode video
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"Video not found: {video_path}")
    with open(video_path, "rb") as f:
        video_bytes = f.read()
    video_base64 = base64.b64encode(video_bytes).decode("utf-8")
    video_data_url = f"data:video/mp4;base64,{video_base64}"

    if not os.path.isfile(image_path):
        raise FileNotFoundError(f"Image file not found: {image_path}")
    with open(image_path, "rb") as image_file:
        base64_image = base64.b64encode(image_file.read()).decode("utf-8")
    image_data_url = f"data:image/png;base64,{base64_image}"

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {"url": video_data_url, "detail": "high"},
                                "media_type": "video/mp4",
                            },
                            {
                                "type": "image_url",
                                "image_url": {"url": image_data_url, "detail": "high"},
                                "media_type": "image/png",
                            },
                        ],
                    }
                ],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
            )
            return completion

        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")
            delay = (2**retry_count) * 0.2 + random.random() * 0.2
            print(f"Retry {retry_count}/{max_retries} after error: {e}, waiting {delay:.2f}s...")
            time.sleep(delay)
    return None


def request_gemini_video_img_token(
    prompt: str, video_path: str, image_path: str, log_id=None, max_tokens: int = 10000, max_retries: int = 10
):
    """
    Makes a multimodal request to the Gemini model using video & ref img + text (Returns Token Usage).
    """
    base_url = cfg("gemini", "base_url")
    # api_version = cfg("gemini", "api_version")
    api_key = cfg("gemini", "api_key")
    model_name = cfg("gemini", "model")

    # ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ base_url åˆå§‹åŒ–æ ‡å‡† OpenAI å®¢æˆ·ç«¯
    client = OpenAI(
        base_url=base_url,
        api_key=api_key,
        timeout=300.0,
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}

    usage_info = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

    # Load and base64-encode video
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"Video not found: {video_path}")
    with open(video_path, "rb") as f:
        video_bytes = f.read()
    video_base64 = base64.b64encode(video_bytes).decode("utf-8")
    video_data_url = f"data:video/mp4;base64,{video_base64}"

    if not os.path.isfile(image_path):
        raise FileNotFoundError(f"Image file not found: {image_path}")
    with open(image_path, "rb") as image_file:
        base64_image = base64.b64encode(image_file.read()).decode("utf-8")
    image_data_url = f"data:image/png;base64,{base64_image}"

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {"url": video_data_url, "detail": "high"},
                                "media_type": "video/mp4",
                            },
                            {
                                "type": "image_url",
                                "image_url": {"url": image_data_url, "detail": "high"},
                                "media_type": "image/png",
                            },
                        ],
                    }
                ],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
            )
            # return completion

            if completion.usage:
                usage_info["prompt_tokens"] = completion.usage.prompt_tokens
                usage_info["completion_tokens"] = completion.usage.completion_tokens
                usage_info["total_tokens"] = completion.usage.total_tokens
            return completion, usage_info

        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")
            delay = (2**retry_count) * 0.2 + random.random() * 0.2
            print(f"Retry {retry_count}/{max_retries} after error: {e}, waiting {delay:.2f}s...")
            time.sleep(delay)
    return None, usage_info


def request_gemini(prompt, log_id=None, max_tokens=8000, max_retries=10):
    """
    Makes a request to the Gemini model via OpenAI-compatible proxy.
    """
    base_url = cfg("gemini", "base_url")
    # api_version = cfg("gemini", "api_version")
    api_key = cfg("gemini", "api_key")
    model_name = cfg("gemini", "model")

    # ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ base_url åˆå§‹åŒ–æ ‡å‡† OpenAI å®¢æˆ·ç«¯
    client = OpenAI(
        base_url=base_url,
        api_key=api_key,
        timeout=300.0,
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
            )
            return completion
        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")

            # Exponential backoff with jitter
            delay = (2**retry_count) * 0.1 + (random.random() * 0.1)
            print(
                f"Request failed with error: {str(e)}. Retrying in {delay:.2f} seconds... (Attempt {retry_count}/{max_retries})"
            )
            time.sleep(delay)


def request_gemini_token(prompt, log_id=None, max_tokens=8000, max_retries=10):
    """
    Makes a request to the Gemini model via OpenAI-compatible proxy (Returns Token Usage).
    """

    base_url = cfg("gemini", "base_url")
    # api_version = cfg("gemini", "api_version")
    api_key = cfg("gemini", "api_key")
    model_name = cfg("gemini", "model")

    # ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ base_url åˆå§‹åŒ–æ ‡å‡† OpenAI å®¢æˆ·ç«¯
    client = OpenAI(
        base_url=base_url,
        api_key=api_key,
        timeout=300.0,
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}

    usage_info = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
            )

            if completion.usage:
                usage_info["prompt_tokens"] = completion.usage.prompt_tokens
                usage_info["completion_tokens"] = completion.usage.completion_tokens
                usage_info["total_tokens"] = completion.usage.total_tokens
            return completion, usage_info

        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")

            # Exponential backoff with jitter
            delay = (2**retry_count) * 0.1 + (random.random() * 0.1)
            print(
                f"Request failed with error: {str(e)}. Retrying in {delay:.2f} seconds... (Attempt {retry_count}/{max_retries})"
            )
            time.sleep(delay)
    return None, usage_info

def request_gpt4o(prompt, log_id=None, max_tokens=8000, max_retries=3):
    """
    Makes a request to the gpt-4o-2024-11-20 model with retry functionality.

    Args:
        prompt (str): The text prompt to send to the model
        log_id (str, optional): The log ID for tracking requests, defaults to tkb+timestamp
        max_tokens (int, optional): Maximum tokens for response, default 8000
        max_retries (int, optional): Maximum number of retry attempts, default 3

    Returns:
        dict: The model's response
    """

    base_url = cfg("gpt4o", "base_url")
    api_version = cfg("gpt4o", "api_version")
    ak = cfg("gpt4o", "api_key")
    model_name = cfg("gpt4o", "model")

    client = openai.AzureOpenAI(
        azure_endpoint=base_url,
        api_version=api_version,
        api_key=ak,
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt,
                            },
                        ],
                    }
                ],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
            )
            return completion.choices[0].message.content
        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")

            # Exponential backoff with jitter
            delay = (2**retry_count) * 0.1 + (random.random() * 0.1)
            print(
                f"Request failed with error: {str(e)}. Retrying in {delay:.2f} seconds... (Attempt {retry_count}/{max_retries})"
            )
            time.sleep(delay)


def request_gpt4o_token(prompt, log_id=None, max_tokens=8000, max_retries=3):
    """
    Makes a request to the gpt-4o model with retry functionality.
    Args:
        prompt (str): The text prompt to send to the model
        log_id (str, optional): The log ID for tracking requests, defaults to tkb+timestamp
        max_tokens (int, optional): Maximum tokens for response, default 8000
        max_retries (int, optional): Maximum number of retry attempts, default 3
    Returns:
        dict: The model's response
    """
    base_url = cfg("gpt4o", "base_url")
    ak = cfg("gpt4o", "api_key")
    model_name = cfg("gpt4o", "model")

    # --- MODIFIED: Use standard OpenAI client & 5 min timeout ---
    client = OpenAI(
        base_url=base_url,
        api_key=ak,
        timeout=300.0,
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}

    usage_info = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt,
                            },
                        ],
                    }
                ],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
                timeout=300.0
            )

            if completion.usage:
                usage_info["prompt_tokens"] = completion.usage.prompt_tokens
                usage_info["completion_tokens"] = completion.usage.completion_tokens
                usage_info["total_tokens"] = completion.usage.total_tokens
            return completion, usage_info

        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                print(f"Failed after {max_retries} attempts. Last error: {str(e)}")
                return None, usage_info

            # Exponential backoff with jitter
            delay = (2**retry_count) * 1.0 + (random.random() * 0.5)
            print(
                f"Request failed with error: {str(e)}. Retrying in {delay:.2f} seconds... (Attempt {retry_count}/{max_retries})"
            )
            time.sleep(delay)
    return None, usage_info


def request_o4mini(prompt, log_id=None, max_tokens=8000, max_retries=3, thinking=False):
    """
    Makes a request to the o4-mini-2025-04-16 model with retry functionality.

    Args:
        prompt (str): The text prompt to send to the model
        log_id (str, optional): The log ID for tracking requests, defaults to tkb+timestamp
        max_tokens (int, optional): Maximum tokens for response, default 8000
        max_retries (int, optional): Maximum number of retry attempts, default 3
        thinking (bool, optional): Whether to enable thinking mode, default False

    Returns:
        dict: The model's response
    """
    base_url = cfg("gpt4omini", "base_url")
    api_version = cfg("gpt4omini", "api_version")
    ak = cfg("gpt4omini", "api_key")
    model_name = cfg("gpt4omini", "model")

    client = openai.AzureOpenAI(
        azure_endpoint=base_url,
        api_version=api_version,
        api_key=ak,
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}

    # Configure extra_body for thinking if enabled
    extra_body = None
    if thinking:
        extra_body = {"thinking": {"type": "enabled", "budget_tokens": 2000}}

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
                extra_body=extra_body,
            )
            return completion
        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")

            # Exponential backoff with jitter
            delay = (2**retry_count) * 0.1 + (random.random() * 0.1)
            print(
                f"Request failed with error: {str(e)}. Retrying in {delay:.2f} seconds... (Attempt {retry_count}/{max_retries})"
            )
            time.sleep(delay)


def request_o4mini_token(prompt, log_id=None, max_tokens=8000, max_retries=3, thinking=False):
    """
    Makes a request to the o4-mini-2025-04-16 model with retry functionality.

    Args:
        prompt (str): The text prompt to send to the model
        log_id (str, optional): The log ID for tracking requests, defaults to tkb+timestamp
        max_tokens (int, optional): Maximum tokens for response, default 8000
        max_retries (int, optional): Maximum number of retry attempts, default 3
        thinking (bool, optional): Whether to enable thinking mode, default False

    Returns:
        dict: The model's response
    """
    base_url = cfg("gpt4omini", "base_url")
    api_version = cfg("gpt4omini", "api_version")
    ak = cfg("gpt4omini", "api_key")
    model_name = cfg("gpt4omini", "model")

    client = openai.AzureOpenAI(
        azure_endpoint=base_url,
        api_version=api_version,
        api_key=ak,
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}

    usage_info = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

    # Configure extra_body for thinking if enabled
    extra_body = None
    if thinking:
        extra_body = {"thinking": {"type": "enabled", "budget_tokens": 2000}}

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
                extra_body=extra_body,
            )

            if completion.usage:
                usage_info["prompt_tokens"] = completion.usage.prompt_tokens
                usage_info["completion_tokens"] = completion.usage.completion_tokens
                usage_info["total_tokens"] = completion.usage.total_tokens
            return completion, usage_info

        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")

            # Exponential backoff with jitter
            delay = (2**retry_count) * 0.1 + (random.random() * 0.1)
            print(
                f"Request failed with error: {str(e)}. Retrying in {delay:.2f} seconds... (Attempt {retry_count}/{max_retries})"
            )
            time.sleep(delay)
    return None, usage_info


def request_gpt5(prompt, log_id=None, max_tokens=1000, max_retries=10):
    """
    Makes a request to the gpt-5 model via standard OpenAI client.
    (No token usage return, just the completion object)
    """
    # 1. è¯»å–é…ç½®
    base_url = cfg("gpt5", "base_url")
    ak = cfg("gpt5", "api_key")
    model_name = cfg("gpt5", "model")

    # 2. âœ… ä¿®æ­£ç‚¹ï¼šæ”¹ä¸ºæ ‡å‡† OpenAI å®¢æˆ·ç«¯
    client = OpenAI(
        base_url=base_url,
        api_key=ak,
        timeout=300.0,
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
                timeout=300.0
            )
            return completion
        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")

            delay = (2**retry_count) * 0.1 + (random.random() * 0.1)
            print(
                f"Request failed with error: {str(e)}. Retrying in {delay:.2f} seconds... (Attempt {retry_count}/{max_retries})"
            )
            time.sleep(delay)

def request_gpt5_token(prompt, log_id=None, max_tokens=1000, max_retries=10):
    """
    Makes a request to the gpt-5 model via standard OpenAI client.
    """
    # 1. è¯»å–é…ç½®
    base_url = cfg("gpt5", "base_url")
    ak = cfg("gpt5", "api_key")
    model_name = cfg("gpt5", "model")

    # 2. âœ… ä¿®æ­£ç‚¹ï¼šæ ‡å‡† OpenAI å®¢æˆ·ç«¯ä½¿ç”¨ base_urlï¼Œè€Œä¸æ˜¯ azure_endpoint
    client = OpenAI(
        base_url=base_url,  # ğŸ‘ˆ æ³¨æ„è¿™é‡Œæ”¹æˆäº† base_url
        api_key=ak,
        timeout=300.0,      # è®¾ç½® 5 åˆ†é’Ÿè¶…æ—¶
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}
    usage_info = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
                timeout=300.0
            )

            if completion.usage:
                usage_info["prompt_tokens"] = completion.usage.prompt_tokens
                usage_info["completion_tokens"] = completion.usage.completion_tokens
                usage_info["total_tokens"] = completion.usage.total_tokens
            return completion, usage_info

        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                print(f"Failed after {max_retries} attempts. Last error: {str(e)}")
                return None, usage_info

            delay = (2**retry_count) * 1.0 + (random.random() * 0.5)
            print(
                f"Request failed with error: {str(e)}. Retrying in {delay:.2f} seconds... (Attempt {retry_count}/{max_retries})"
            )
            time.sleep(delay)
    return None, usage_info

def request_gpt5_img(prompt, image_path=None, log_id=None, max_tokens=1000, max_retries=10):
    """
    Makes a request to the gpt-5 model with optional image input.
    Uses standard OpenAI client.
    """
    # 1. è¯»å–é…ç½®
    base_url = cfg("gpt5", "base_url")
    ak = cfg("gpt5", "api_key")
    model_name = cfg("gpt5", "model")

    # 2. åˆå§‹åŒ–æ ‡å‡†å®¢æˆ·ç«¯
    client = OpenAI(
        base_url=base_url,
        api_key=ak,
        timeout=300.0,
    )
    
    if log_id is None:
        log_id = generate_log_id()
    
    # éƒ¨åˆ†ä¸­è½¬å•†å¯èƒ½ä¸æ”¯æŒè‡ªå®šä¹‰ headerï¼Œå¦‚æœæŠ¥é”™å¯æ³¨é‡Šæ‰
    extra_headers = {"X-TT-LOGID": log_id}

    # 3. æ„å»ºæ¶ˆæ¯ä½“
    if image_path:
        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
        if not os.path.isfile(image_path):
            raise FileNotFoundError(f"Image file not found: {image_path}")

        # è¯»å–å¹¶è½¬ä¸º Base64
        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode("utf-8")

        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url", 
                        "image_url": {
                            "url": f"data:image/png;base64,{base64_image}",
                            "detail": "high" # å¼ºåˆ¶é«˜æ¸…æ¨¡å¼
                        }
                    },
                ],
            }
        ]
    else:
        # å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œå°±å½“æ™®é€šå¯¹è¯å¤„ç†
        messages = [{"role": "user", "content": prompt}]

    # 4. å‘é€è¯·æ±‚
    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=messages,
                max_tokens=max_tokens,
                extra_headers=extra_headers,
                timeout=300.0
            )
            return completion
            
        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")
            
            delay = (2**retry_count) * 1.0 + (random.random() * 0.5)
            print(
                f"Request failed with error: {str(e)}. Retrying in {delay:.2f} seconds... (Attempt {retry_count}/{max_retries})"
            )
            time.sleep(delay)

def request_gpt5_with_video(prompt: str, video_path: str, log_id=None, max_tokens: int = 10000, max_retries: int = 10):
    """
    [GPT-5] Video + Text Request.
    Mimics Gemini's video handling. 
    Note: Standard OpenAI models usually expect frames, but this sends base64 video stream 
    relying on the proxy/model's native multimodal capabilities.
    """
    base_url = cfg("gpt5", "base_url")
    api_key = cfg("gpt5", "api_key")
    model_name = cfg("gpt5", "model")

    client = OpenAI(
        base_url=base_url,
        api_key=api_key,
        timeout=600.0, # è§†é¢‘å¤„ç†é€šå¸¸éœ€è¦æ›´é•¿æ—¶é—´ï¼Œå»ºè®®è®¾ä¸º 600s
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}

    # Load and base64-encode video
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"Video not found: {video_path}")

    with open(video_path, "rb") as f:
        video_bytes = f.read()

    video_base64 = base64.b64encode(video_bytes).decode("utf-8")
    data_url = f"data:video/mp4;base64,{video_base64}"

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            # ä»¿ç…§ Gemini çš„ç»“æ„å‘é€è§†é¢‘
                            # æ³¨æ„ï¼šå¦‚æœæ ‡å‡† GPT-4o æŠ¥é”™ï¼Œè¿™é‡Œå¯èƒ½éœ€è¦æ”¹ä¸ºå‘é€å›¾ç‰‡å¸§åˆ—è¡¨
                            {
                                "type": "image_url", 
                                "image_url": {"url": data_url, "detail": "high"}, 
                                # è¿™é‡Œçš„ media_type æ˜¯ä¸ºäº†å…¼å®¹éƒ¨åˆ†ä¸­è½¬ç«™å¯¹ Gemini æ ¼å¼çš„è¯†åˆ«
                                # æ ‡å‡† OpenAI åº“å¯èƒ½ä¼šå¿½ç•¥è¿™ä¸ªé¢å¤–å­—æ®µï¼Œä½†åœ¨ payload ä¸­ä¼šä¿ç•™
                            },
                        ],
                    }
                ],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
                timeout=600.0
            )
            return completion

        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")
            
            delay = (2**retry_count) * 0.5 + (random.random() * 0.5)
            print(f"GPT-5 Video Retry {retry_count}/{max_retries}: {e}, waiting {delay:.2f}s...")
            time.sleep(delay)


def request_gpt5_video_img(
    prompt: str, video_path: str, image_path: str, log_id=None, max_tokens: int = 10000, max_retries: int = 10
):
    """
    [GPT-5] Video + Reference Image + Text Request.
    Mimics request_gemini_video_img.
    """
    base_url = cfg("gpt5", "base_url")
    api_key = cfg("gpt5", "api_key")
    model_name = cfg("gpt5", "model")

    client = OpenAI(
        base_url=base_url,
        api_key=api_key,
        timeout=600.0,
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}

    # 1. Process Video
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"Video not found: {video_path}")
    with open(video_path, "rb") as f:
        video_bytes = f.read()
    video_base64 = base64.b64encode(video_bytes).decode("utf-8")
    video_data_url = f"data:video/mp4;base64,{video_base64}"

    # 2. Process Image
    if not os.path.isfile(image_path):
        raise FileNotFoundError(f"Image file not found: {image_path}")
    with open(image_path, "rb") as image_file:
        base64_image = base64.b64encode(image_file.read()).decode("utf-8")
    image_data_url = f"data:image/png;base64,{base64_image}"

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {"url": video_data_url, "detail": "high"},
                            },
                            {
                                "type": "image_url",
                                "image_url": {"url": image_data_url, "detail": "high"},
                            },
                        ],
                    }
                ],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
                timeout=600.0
            )
            return completion

        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")
            delay = (2**retry_count) * 0.5 + (random.random() * 0.5)
            print(f"GPT-5 Video+Img Retry {retry_count}/{max_retries}: {e}, waiting {delay:.2f}s...")
            time.sleep(delay)
    return None


def request_gpt5_video_img_token(
    prompt: str, video_path: str, image_path: str, log_id=None, max_tokens: int = 10000, max_retries: int = 10
):
    """
    [GPT-5] Video + Reference Image + Text Request (Returns Token Usage).
    Mimics request_gemini_video_img_token.
    """
    base_url = cfg("gpt5", "base_url")
    api_key = cfg("gpt5", "api_key")
    model_name = cfg("gpt5", "model")

    client = OpenAI(
        base_url=base_url,
        api_key=api_key,
        timeout=600.0,
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}
    usage_info = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

    # 1. Process Video
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"Video not found: {video_path}")
    with open(video_path, "rb") as f:
        video_bytes = f.read()
    video_base64 = base64.b64encode(video_bytes).decode("utf-8")
    video_data_url = f"data:video/mp4;base64,{video_base64}"

    # 2. Process Image
    if not os.path.isfile(image_path):
        raise FileNotFoundError(f"Image file not found: {image_path}")
    with open(image_path, "rb") as image_file:
        base64_image = base64.b64encode(image_file.read()).decode("utf-8")
    image_data_url = f"data:image/png;base64,{base64_image}"

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {"url": video_data_url, "detail": "high"},
                            },
                            {
                                "type": "image_url",
                                "image_url": {"url": image_data_url, "detail": "high"},
                            },
                        ],
                    }
                ],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
                timeout=600.0
            )
            
            if completion.usage:
                usage_info["prompt_tokens"] = completion.usage.prompt_tokens
                usage_info["completion_tokens"] = completion.usage.completion_tokens
                usage_info["total_tokens"] = completion.usage.total_tokens
            return completion, usage_info

        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")
            delay = (2**retry_count) * 0.5 + (random.random() * 0.5)
            print(f"GPT-5 Video+Img+Token Retry {retry_count}/{max_retries}: {e}, waiting {delay:.2f}s...")
            time.sleep(delay)
    return None, usage_info

def request_gpt41(prompt, log_id=None, max_tokens=1000, max_retries=3):
    """
    Makes a request to the gpt-4.1-2025-04-14 model with retry functionality.

    Args:
        prompt (str): The text prompt to send to the model
        log_id (str, optional): The log ID for tracking requests, defaults to tkb+timestamp
        max_tokens (int, optional): Maximum tokens for response, default 1000
        max_retries (int, optional): Maximum number of retry attempts, default 3

    Returns:
        dict: The model's response
    """
    base_url = cfg("gpt41", "base_url")
    api_version = cfg("gpt41", "api_version")
    api_key = cfg("gpt41", "api_key")
    model_name = cfg("gpt41", "model")

    client = openai.AzureOpenAI(
        azure_endpoint=base_url,
        api_version=api_version,
        api_key=api_key,
    )

    if log_id is None:
        log_id = generate_log_id()

    extra_headers = {"X-TT-LOGID": log_id}

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
            )
            return completion
        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")

            # Exponential backoff with jitter
            delay = (2**retry_count) * 0.1 + (random.random() * 0.1)
            print(
                f"Request failed with error: {str(e)}. Retrying in {delay:.2f} seconds... (Attempt {retry_count}/{max_retries})"
            )
            time.sleep(delay)


def request_gpt41_token(prompt, log_id=None, max_tokens=1000, max_retries=3):
    # è¯»å–é…ç½®
    base_url = cfg("gpt41", "base_url")
    ak = cfg("gpt41", "api_key")
    model_name = cfg("gpt41", "model")

    # --- MODIFIED: Use standard OpenAI client & 5 min timeout ---
    client = OpenAI(
        base_url=base_url,
        api_key=ak,
        timeout=300.0,
    )
    # ------------------------------------

    if log_id is None:
        log_id = generate_log_id()

    # æŸäº›ä¸­è½¬ç«™ä¸æ”¯æŒè‡ªå®šä¹‰ headerï¼Œå¦‚æœæŠ¥é”™å¯ä»¥æŠŠ extra_headers åˆ æ‰
    extra_headers = {"X-TT-LOGID": log_id} 
    usage_info = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                extra_headers=extra_headers,
                timeout=300.0
            )

            if completion.usage:
                usage_info["prompt_tokens"] = completion.usage.prompt_tokens
                usage_info["completion_tokens"] = completion.usage.completion_tokens
                usage_info["total_tokens"] = completion.usage.total_tokens
            return completion, usage_info

        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                # å³ä½¿å¤±è´¥ä¹Ÿè¿”å›ï¼Œä»¥å…ç¨‹åºå´©æºƒ
                print(f"Failed after {max_retries} attempts. Last error: {str(e)}")
                return None, usage_info
            
            # å¢åŠ é‡è¯•ç­‰å¾…æ—¶é—´
            delay = (2**retry_count) * 1.0 + (random.random() * 0.5)
            print(f"Retry {retry_count} error: {str(e)}. Waiting {delay:.2f}s...")
            time.sleep(delay)

    return None, usage_info


def request_gpt41_img(prompt, image_path=None, log_id=None, max_tokens=1000, max_retries=3):
    """
    Makes a request to the gpt-4.1-2025-04-14 model with optional image input and retry functionality.
    Args:
        prompt (str): The text prompt to send to the model
        image_path (str, optional): Absolute path to an image file to include
        log_id (str, optional): The log ID for tracking requests, defaults to tkb+timestamp
        max_tokens (int, optional): Maximum tokens for response, default 1000
        max_retries (int, optional): Maximum number of retry attempts, default 3
    Returns:
        dict: The model's response
    """
    base_url = cfg("gpt41", "base_url")
    api_version = cfg("gpt41", "api_version")
    ak = cfg("gpt41", "api_key")
    model_name = cfg("gpt41", "model")

    client = openai.AzureOpenAI(
        azure_endpoint=base_url,
        api_version=api_version,
        api_key=ak,
    )
    if log_id is None:
        log_id = generate_log_id()
    extra_headers = {"X-TT-LOGID": log_id}

    if image_path:
        # æ£€æŸ¥å›¾ç‰‡è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.isfile(image_path):
            raise FileNotFoundError(f"Image file not found: {image_path}")

        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode("utf-8")

        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}},
                ],
            }
        ]

    else:
        messages = [{"role": "user", "content": prompt}]
    retry_count = 0
    while retry_count < max_retries:
        try:
            completion = client.chat.completions.create(
                model=model_name,
                messages=messages,
                max_tokens=max_tokens,
                extra_headers=extra_headers,
            )
            return completion
        except Exception as e:
            retry_count += 1
            if retry_count >= max_retries:
                raise Exception(f"Failed after {max_retries} attempts. Last error: {str(e)}")
            delay = (2**retry_count) * 0.1 + (random.random() * 0.1)
            print(
                f"Request failed with error: {str(e)}. Retrying in {delay:.2f} seconds... (Attempt {retry_count}/{max_retries})"
            )
            time.sleep(delay)


if __name__ == "__main__":

    # Gemini
    # response_gemini = request_gemini("ä¸Šæµ·å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
    # print(response_gemini.model_dump_json())

    # # GPT-4o
    # response_gpt4o = request_gpt4o("ä¸Šæµ·å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
    # print(response_gpt4o)

    # # o4-mini
    # response_o4mini = request_o4mini("ä¸Šæµ·å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
    # print(response_o4mini.model_dump_json())

    # # GPT-4.1
    #response_gpt41 = request_gpt41("ä¸Šæµ·å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
    #print(response_gpt41.model_dump_json())

    # GPT-5
    # response_gpt5 = request_gpt5("æ–°åŠ å¡å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
    # print(response_gpt5.model_dump_json())

    # Claude
    response_claude = request_claude_token("æ–°åŠ å¡å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
    print(response_claude)
    
    # æµ‹è¯• prompt
    # print("\nğŸš€ å¼€å§‹ã€æ··åˆæ¶æ„ã€‘å…¨åŠŸèƒ½æµ‹è¯• (Hybrid Agent Debug)...")
    # print("ğŸ¯ ç›®æ ‡æ¶æ„: GPT-5 (å¤§è„‘/ä»£ç ) + Gemini (çœ¼ç›/è§†é¢‘)")
    # print("=" * 60)

    # # ==========================================
    # # 1. æµ‹è¯• GPT-5 (å¤§è„‘/ä»£ç ç”Ÿæˆèƒ½åŠ›)
    # # ==========================================
    # print("1ï¸âƒ£ [å¤§è„‘æµ‹è¯•] æ­£åœ¨è¯·æ±‚ GPT-5 (request_gpt5_token) ...")
    # prompt_text = "ä½ å¥½ï¼Œè¯·ç”¨ä¸­æ–‡ç®€çŸ­ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼Œå¹¶å†™ä¸€ä¸ªç®€å•çš„Python Hello World å‡½æ•°ã€‚"
    
    # try:
    #     start_time = time.time()
    #     # è°ƒç”¨ GPT-5 æ¥å£
    #     response, usage = request_gpt5_token(prompt_text)
    #     duration = time.time() - start_time
        
    #     if response:
    #         print(f"âœ… GPT-5 è¯·æ±‚æˆåŠŸ (è€—æ—¶ {duration:.2f}s)")
    #         # è§£æå†…å®¹
    #         try:
    #             content = response.choices[0].message.content
    #             # ğŸ”´ ä¿®æ”¹ç‚¹ï¼šå»æ‰äº† [:100]ï¼Œæ‰“å°å®Œæ•´å†…å®¹
    #             print(f"ğŸ’¬ æ¨¡å‹å›å¤:\n{content.strip()}") 
    #         except Exception:
    #             print(f"âš ï¸ æ— æ³•è§£æå›å¤å†…å®¹ï¼ŒåŸå§‹å¯¹è±¡: {response}")
    #         print(f"ğŸ“Š Tokenæ•°æ®: {usage}")
    #     else:
    #         print("âŒ GPT-5 è¯·æ±‚å¤±è´¥: è¿”å›ä¸ºç©º")
            
    # except Exception as e:
    #     print(f"âŒ GPT-5 æµ‹è¯•å‘ç”Ÿå¼‚å¸¸: {e}")
    
    # print("-" * 60)

    # # ==========================================
    # # 2. æµ‹è¯• Gemini (çœ¼ç›/è§†é¢‘ç†è§£èƒ½åŠ›)
    # # ==========================================
    # print("2ï¸âƒ£ [çœ¼ç›æµ‹è¯•] æ­£åœ¨è¯·æ±‚ Gemini (request_gemini_video_img_token) ...")
    
    # # è‡ªåŠ¨å®šä½é¡¹ç›®ä¸­çš„æµ‹è¯•èµ„æº
    # current_dir = pathlib.Path(__file__).parent.resolve()
    
    # # 1. å¯»æ‰¾ä¸€å¼ å­˜åœ¨çš„å›¾ç‰‡
    # image_path = current_dir / "assets" / "reference" / "GRID.png"
    # if not image_path.exists():
    #     image_path = current_dir / "assets" / "icon" / "cat.png"

    # # 2. è®¾ç½®è§†é¢‘è·¯å¾„ 
    # video_path = current_dir / "CASES" / "test_video.mp4" 

    # print(f"ğŸ“‚ å›¾ç‰‡è·¯å¾„: {image_path}")
    # print(f"ğŸ“‚ è§†é¢‘è·¯å¾„: {video_path}")

    # if image_path.exists() and video_path.exists():
    #     print("â–¶ï¸ æ–‡ä»¶å­˜åœ¨ï¼Œå¼€å§‹å‘é€å¤šæ¨¡æ€è¯·æ±‚ (Gemini)...")
    #     prompt_mm = "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼Œå¹¶åˆ†æè§†é¢‘ä¸­å‘ç”Ÿçš„äº‹æƒ…ã€‚"
        
    #     try:
    #         start_time = time.time()
    #         # è°ƒç”¨ Gemini å¤šæ¨¡æ€æ¥å£
    #         response_mm, usage_mm = request_gemini_video_img_token(prompt_mm, str(video_path), str(image_path))
    #         duration = time.time() - start_time
            
    #         if response_mm:
    #             print(f"âœ… Gemini å¤šæ¨¡æ€è¯·æ±‚æˆåŠŸ (è€—æ—¶ {duration:.2f}s)")
    #             try:
    #                 # å…¼å®¹ä¸åŒæ ¼å¼çš„è§£æ
    #                 if hasattr(response_mm, 'choices'):
    #                     content_mm = response_mm.choices[0].message.content
    #                 elif hasattr(response_mm, 'candidates'):
    #                     content_mm = response_mm.candidates[0].content.parts[0].text
    #                 else:
    #                     content_mm = str(response_mm)
                    
    #                 # ğŸ”´ ä¿®æ”¹ç‚¹ï¼šå»æ‰äº† [:100]ï¼Œæ‰“å°å®Œæ•´å†…å®¹
    #                 print(f"ğŸ’¬ æ¨¡å‹å›å¤:\n{content_mm.strip()}") 
    #             except Exception:
    #                 print(f"âš ï¸ æ— æ³•è§£æå›å¤å†…å®¹")
    #             print(f"ğŸ“Š Tokenæ•°æ®: {usage_mm}")
    #         else:
    #             print("âŒ Gemini å¤šæ¨¡æ€è¯·æ±‚å¤±è´¥: è¿”å›ä¸ºç©º")
    #     except Exception as e:
    #         print(f"âŒ Gemini å¤šæ¨¡æ€æµ‹è¯•å‘ç”Ÿå¼‚å¸¸: {e}")
    # else:
    #     print("âš ï¸ è·³è¿‡ Gemini æµ‹è¯•: æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶ (test_video.mp4 æˆ– å›¾ç‰‡)ã€‚")

    # print("=" * 60)
    # print("ğŸš€ æµ‹è¯•ç»“æŸã€‚å¦‚æœä»¥ä¸Šä¸¤æ­¥éƒ½æˆåŠŸï¼Œæ‚¨å¯ä»¥æ”¾å¿ƒè¿è¡Œ agent.py æ··åˆä»»åŠ¡äº†ã€‚")